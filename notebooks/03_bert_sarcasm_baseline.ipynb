{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNtOfqOXJD0p+iE6SeUF+3c"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# BERT Baseline for Figurative Language Understanding\n",
        "\n",
        "In this notebook, we prepare a BERT-based baseline model for the BESSTIE dataset.\n",
        "We use the preprocessed encoder-ready data produced earlier and focus on:\n",
        "\n",
        "- Loading preprocessed tensors\n",
        "- Understanding their structure\n",
        "- Preparing datasets and dataloaders\n",
        "- Verifying that everything is ready for training"
      ],
      "metadata": {
        "id": "iIib-saP1G2S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports and environment setup\n",
        "\n",
        "We import the libraries required for:\n",
        "- loading PyTorch tensors\n",
        "- building datasets and dataloaders\n",
        "- defining a BERT classification model\n",
        "- basic evaluation utilities\n",
        "\n",
        "No training is performed yet."
      ],
      "metadata": {
        "id": "a99lIjbg1Qqg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pw0jQatAstgU"
      },
      "outputs": [],
      "source": [
        "# Core libraries\n",
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# PyTorch utilities\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Hugging Face model\n",
        "from transformers import BertForSequenceClassification\n",
        "\n",
        "# Metrics\n",
        "from sklearn.metrics import accuracy_score, f1_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Torch version:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())"
      ],
      "metadata": {
        "id": "5VPfHWpz1oJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reproducibility\n",
        "\n",
        "We fix random seeds to ensure reproducible results.\n",
        "This is important for fair comparison with other models and experiments."
      ],
      "metadata": {
        "id": "Pdb2rEUw1yZQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fix random seeds\n",
        "SEED = 50\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "print(\"Random seeds fixed.\")"
      ],
      "metadata": {
        "id": "M5glEZ0i1wMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple check: generate a random number twice\n",
        "print(np.random.rand())\n",
        "print(np.random.rand())"
      ],
      "metadata": {
        "id": "inJV49WY11Gl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading preprocessed encoder data\n",
        "\n",
        "We load the encoder-ready tensors created during preprocessing.\n",
        "These tensors already include:\n",
        "- input_ids\n",
        "- attention_mask\n",
        "- labels\n",
        "\n",
        "The data is split into train, validation, and test sets."
      ],
      "metadata": {
        "id": "lsbYbbak14iK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "print(\"Google Drive mounted.\")"
      ],
      "metadata": {
        "id": "DOmUtDA-2yXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to preprocessed data\n",
        "DATA_PATH = \"/content/drive/MyDrive/DNLP/data/processed_data/sarcasm_bert_encoder.pt\"\n",
        "\n",
        "# Load the data\n",
        "encoder_data = torch.load(DATA_PATH)\n",
        "\n",
        "print(\"Keys in loaded data:\", encoder_data.keys())"
      ],
      "metadata": {
        "id": "d58ptfWK12w1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check split sizes\n",
        "for split in encoder_data:\n",
        "    print(\n",
        "        split,\n",
        "        encoder_data[split][\"input_ids\"].shape,\n",
        "        encoder_data[split][\"labels\"].shape\n",
        "    )"
      ],
      "metadata": {
        "id": "t_KTBmkv17b3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inspecting tensor structure\n",
        "\n",
        "Before modeling, we inspect the shape and content of tensors\n",
        "to confirm that preprocessing was applied correctly.\n",
        "\n",
        "We check:\n",
        "- sequence length (max_length)\n",
        "- label format\n",
        "- attention masks"
      ],
      "metadata": {
        "id": "SDeyT0mz3nfM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect one training example\n",
        "sample_input_ids = encoder_data[\"train\"][\"input_ids\"][0]\n",
        "sample_attention_mask = encoder_data[\"train\"][\"attention_mask\"][0]\n",
        "sample_label = encoder_data[\"train\"][\"labels\"][0]\n",
        "\n",
        "print(\"Input IDs shape:\", sample_input_ids.shape)\n",
        "print(\"Attention mask shape:\", sample_attention_mask.shape)\n",
        "print(\"Label:\", sample_label)"
      ],
      "metadata": {
        "id": "Lp76qUQs3hhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_input_ids"
      ],
      "metadata": {
        "id": "ujmnhUso3qFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_attention_mask"
      ],
      "metadata": {
        "id": "FUIL4ZmT3tfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_label"
      ],
      "metadata": {
        "id": "BxxW3rl03wx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify attention mask consistency\n",
        "print(\"Number of real tokens:\", sample_attention_mask.sum().item())\n",
        "print(\"Total sequence length:\", sample_attention_mask.shape[0])"
      ],
      "metadata": {
        "id": "YCL7asEr3zZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset class\n",
        "\n",
        "We define a simple PyTorch Dataset to wrap the preprocessed tensors.\n",
        "This allows us to use PyTorch DataLoader for batching.\n",
        "\n",
        "The dataset does NOT perform tokenization.\n",
        "It only returns already-prepared tensors."
      ],
      "metadata": {
        "id": "eDVFJxr238P0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BertDataset(Dataset):\n",
        "    def __init__(self, split_data):\n",
        "        self.input_ids = split_data[\"input_ids\"]\n",
        "        self.attention_mask = split_data[\"attention_mask\"]\n",
        "        self.labels = split_data[\"labels\"]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            \"input_ids\": self.input_ids[idx],\n",
        "            \"attention_mask\": self.attention_mask[idx],\n",
        "            \"labels\": self.labels[idx]\n",
        "        }"
      ],
      "metadata": {
        "id": "2OBL1zAi35FV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = BertDataset(encoder_data[\"train\"])\n",
        "print(\"Number of training samples:\", len(train_dataset))\n",
        "print(\"Keys returned by dataset:\", train_dataset[0].keys())"
      ],
      "metadata": {
        "id": "mMW4RFbk4CpC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DataLoaders\n",
        "\n",
        "We create DataLoaders for batching.\n",
        "At this stage, we only test small batches to verify correctness."
      ],
      "metadata": {
        "id": "RIZLYjZC4G5s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 8\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "print(\"Train DataLoader ready.\")"
      ],
      "metadata": {
        "id": "b_vpLa8m4EY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch one batch\n",
        "batch = next(iter(train_loader))\n",
        "\n",
        "for key in batch:\n",
        "    print(key, batch[key].shape)"
      ],
      "metadata": {
        "id": "VY-kTRWt4Jij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading BERT for sequence classification\n",
        "\n",
        "We load a pretrained BERT model with a classification head.\n",
        "The model is configured for binary classification.\n",
        "\n",
        "No training is performed in this cell."
      ],
      "metadata": {
        "id": "MNRIYnXZ4Pje"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\",\n",
        "    num_labels=2\n",
        ")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "print(\"Model loaded on device:\", device)"
      ],
      "metadata": {
        "id": "YfKBcZog4LXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count trainable parameters\n",
        "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Trainable parameters: {num_params:,}\")"
      ],
      "metadata": {
        "id": "5UVq5du44S75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sanity check: forward pass\n",
        "\n",
        "Before training, we perform a single forward pass on one batch.\n",
        "This ensures:\n",
        "- no shape mismatch\n",
        "- no device errors\n",
        "- correct output format"
      ],
      "metadata": {
        "id": "E3U7M6D04XuZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "batch = next(iter(train_loader))\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(\n",
        "        input_ids=batch[\"input_ids\"].to(device),\n",
        "        attention_mask=batch[\"attention_mask\"].to(device),\n",
        "        labels=batch[\"labels\"].to(device)\n",
        "    )\n",
        "\n",
        "print(\"Loss:\", outputs.loss.item())\n",
        "print(\"Logits shape:\", outputs.logits.shape)"
      ],
      "metadata": {
        "id": "FtV4tX-s4Vs-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check that logits match expected shape\n",
        "assert outputs.logits.shape[1] == 2\n",
        "print(\"Forward pass successful. Shapes are correct.\")"
      ],
      "metadata": {
        "id": "OapDJGxz4cNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Status\n",
        "\n",
        "At this point:\n",
        "- Data loading is correct\n",
        "- Tensor shapes are consistent\n",
        "- DataLoaders work as expected\n",
        "- BERT forward pass runs without errors\n",
        "\n",
        "The notebook is ready for:\n",
        "- defining loss functions (weighted or focal)\n",
        "- training loop\n",
        "- evaluation\n",
        "\n",
        "We intentionally stop here before any heavy computation."
      ],
      "metadata": {
        "id": "JELiDO264mzd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading precomputed class weights\n",
        "\n",
        "Class weights were computed during preprocessing and saved to ensure\n",
        "consistent handling of class imbalance across experiments.\n",
        "\n",
        "We load and reuse these weights here."
      ],
      "metadata": {
        "id": "YtcorYVh7pP0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load class weights saved during preprocessing\n",
        "WEIGHTS_PATH = \"/content/drive/MyDrive/DNLP/data/processed_data/sarcasm_weights.pt\"\n",
        "class_weights = torch.load(WEIGHTS_PATH).to(device)\n",
        "\n",
        "print(\"Loaded class weights:\", class_weights)"
      ],
      "metadata": {
        "id": "Ig56dHjt4g_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert class_weights.shape[0] == 2\n",
        "assert class_weights.min() > 0\n",
        "print(\"Precomputed class weights verified.\")"
      ],
      "metadata": {
        "id": "fl9-uPzN7rpK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Weighted Cross-Entropy Loss\n",
        "\n",
        "This is the standard loss used to handle class imbalance.\n",
        "Misclassifying minority-class samples results in a higher penalty."
      ],
      "metadata": {
        "id": "64Ju886O8QMG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weighted_loss_fn = torch.nn.CrossEntropyLoss(\n",
        "    weight=class_weights.to(device)\n",
        ")\n",
        "\n",
        "print(\"Weighted loss function initialized.\")"
      ],
      "metadata": {
        "id": "RgqYn80B8K6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test loss on dummy logits\n",
        "dummy_logits = torch.tensor([[2.0, 0.5]], device=device)\n",
        "dummy_label = torch.tensor([1], device=device)\n",
        "\n",
        "loss_value = weighted_loss_fn(dummy_logits, dummy_label)\n",
        "print(\"Dummy weighted loss:\", loss_value.item())"
      ],
      "metadata": {
        "id": "rQN6mbS68SWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimizer setup\n",
        "\n",
        "We define the optimizer following the reference paper settings.\n",
        "The optimizer is defined but not used yet."
      ],
      "metadata": {
        "id": "pVFrBtSi8ZWJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import AdamW\n",
        "\n",
        "optimizer = AdamW(\n",
        "    model.parameters(),\n",
        "    lr=2e-5,\n",
        "    weight_decay=0.01\n",
        ")\n",
        "\n",
        "print(\"Optimizer initialized.\")"
      ],
      "metadata": {
        "id": "lrT2ftcO8T_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check optimizer has parameters\n",
        "param_groups = sum(len(g[\"params\"]) for g in optimizer.param_groups)\n",
        "print(\"Number of parameter groups:\", param_groups)"
      ],
      "metadata": {
        "id": "Vm7yUNjP8bUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation metrics\n",
        "\n",
        "We define helper functions to compute accuracy and macro F1.\n",
        "These metrics are used consistently across experiments."
      ],
      "metadata": {
        "id": "qoPObB_i83Qy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(logits, labels):\n",
        "    preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "    labels = labels.cpu().numpy()\n",
        "\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    f1 = f1_score(labels, preds, average=\"macro\")\n",
        "\n",
        "    return acc, f1"
      ],
      "metadata": {
        "id": "mvBbf2Qw80gp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_logits = torch.tensor([[0.2, 0.8], [0.6, 0.4]])\n",
        "dummy_labels = torch.tensor([1, 0])\n",
        "\n",
        "acc, f1 = compute_metrics(dummy_logits, dummy_labels)\n",
        "print(\"Accuracy:\", acc)\n",
        "print(\"Macro F1:\", f1)"
      ],
      "metadata": {
        "id": "z4HydpqC86Vm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sanity check: metrics on one batch\n",
        "\n",
        "Before full training, we test metric computation on a single batch.\n",
        "This ensures that logits, labels, and metrics are compatible."
      ],
      "metadata": {
        "id": "kSRitzki8_M2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "batch = next(iter(train_loader))\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(\n",
        "        input_ids=batch[\"input_ids\"].to(device),\n",
        "        attention_mask=batch[\"attention_mask\"].to(device)\n",
        "    )\n",
        "\n",
        "acc, f1 = compute_metrics(\n",
        "    outputs.logits,\n",
        "    batch[\"labels\"].to(device)\n",
        ")\n",
        "\n",
        "print(\"Batch accuracy:\", acc)\n",
        "print(\"Batch macro F1:\", f1)"
      ],
      "metadata": {
        "id": "AD7D8kQi87m4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert 0.0 <= acc <= 1.0\n",
        "assert 0.0 <= f1 <= 1.0\n",
        "print(\"Metric computation successful.\")"
      ],
      "metadata": {
        "id": "zET_KfQW9A02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training configuration\n",
        "\n",
        "We define all training hyperparameters in one place.\n",
        "This makes experiments easy to reproduce and modify."
      ],
      "metadata": {
        "id": "iMvoTQKt9U6J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training configuration\n",
        "NUM_EPOCHS = 1          # sanity run (will increase later)\n",
        "LEARNING_RATE = 2e-5\n",
        "BATCH_SIZE = 8\n",
        "LOG_INTERVAL = 10       # how often to print loss\n",
        "\n",
        "print(\"Training configuration set.\")"
      ],
      "metadata": {
        "id": "1NWLJHjq9DVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert NUM_EPOCHS >= 1\n",
        "assert LEARNING_RATE > 0\n",
        "print(\"Training config verified.\")"
      ],
      "metadata": {
        "id": "4IGg014W9dPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training loop\n",
        "\n",
        "We define a simple training loop for BERT.\n",
        "At this stage, we only support:\n",
        "- weighted cross-entropy loss\n",
        "- single-GPU or CPU training\n",
        "\n",
        "This loop will be reused for all experiments."
      ],
      "metadata": {
        "id": "DCSKAfF49hL2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, dataloader, optimizer, loss_fn, device):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for step, batch in enumerate(dataloader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Move data to device\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "\n",
        "        # Compute loss\n",
        "        loss = loss_fn(outputs.logits, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Log progress\n",
        "        if step % LOG_INTERVAL == 0:\n",
        "            print(f\"Step {step} - Loss: {loss.item():.4f}\")\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    return avg_loss"
      ],
      "metadata": {
        "id": "43ETz9Z69e-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training loop function defined.\")"
      ],
      "metadata": {
        "id": "U99UgO4d9lDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Validation loop\n",
        "\n",
        "The validation loop evaluates the model without updating weights.\n",
        "We compute accuracy and macro F1."
      ],
      "metadata": {
        "id": "PYwTsZDK9otv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, dataloader, device):\n",
        "    model.eval()\n",
        "\n",
        "    all_logits = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask\n",
        "            )\n",
        "\n",
        "            all_logits.append(outputs.logits)\n",
        "            all_labels.append(labels)\n",
        "\n",
        "    all_logits = torch.cat(all_logits, dim=0)\n",
        "    all_labels = torch.cat(all_labels, dim=0)\n",
        "\n",
        "    acc, f1 = compute_metrics(all_logits, all_labels)\n",
        "    return acc, f1"
      ],
      "metadata": {
        "id": "C_0V9ceA9mj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Evaluation loop function defined.\")"
      ],
      "metadata": {
        "id": "-3PSWEkn9rdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Validation DataLoader\n",
        "\n",
        "We prepare a DataLoader for the validation split."
      ],
      "metadata": {
        "id": "lz9xZZOP9xID"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset = BertDataset(encoder_data[\"val\"])\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "print(\"Validation DataLoader ready.\")"
      ],
      "metadata": {
        "id": "UyiVKBUT9tRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch = next(iter(val_loader))\n",
        "for k in batch:\n",
        "    print(k, batch[k].shape)"
      ],
      "metadata": {
        "id": "WikEvEOg9zFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Time logging utilities\n",
        "\n",
        "We define helper functions to log elapsed time during training.\n",
        "This helps us understand computational cost and compare experiments."
      ],
      "metadata": {
        "id": "qBHWPnlp-tiI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def format_time(seconds):\n",
        "    \"\"\"Convert seconds to hh:mm:ss format.\"\"\"\n",
        "    hours = int(seconds // 3600)\n",
        "    minutes = int((seconds % 3600) // 60)\n",
        "    secs = int(seconds % 60)\n",
        "    return f\"{hours:02d}:{minutes:02d}:{secs:02d}\""
      ],
      "metadata": {
        "id": "592s5qVZ-t_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Full training configuration\n",
        "\n",
        "We now switch from a sanity run to full training.\n",
        "These settings follow the reference paper as closely as possible."
      ],
      "metadata": {
        "id": "FAQWHaXO-1LT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Full training settings\n",
        "NUM_EPOCHS = 30          # paper-aligned\n",
        "LOG_INTERVAL = 100       # less verbose during full training\n",
        "\n",
        "print(f\"Training for {NUM_EPOCHS} epochs.\")"
      ],
      "metadata": {
        "id": "fI8DVSQz-ztC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# Full Training Script with tqdm\n",
        "# ================================\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "import time\n",
        "\n",
        "# ----------------\n",
        "# Training setup\n",
        "# ----------------\n",
        "NUM_EPOCHS = 30\n",
        "\n",
        "history = {\n",
        "    \"train_loss\": [],\n",
        "    \"val_acc\": [],\n",
        "    \"val_f1\": []\n",
        "}\n",
        "\n",
        "print(\"Starting full training...\\n\")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# tqdm progress bar (installer-style)\n",
        "epoch_bar = tqdm(\n",
        "    range(1, NUM_EPOCHS + 1),\n",
        "    desc=\"Installing BERT baseline\",\n",
        "    unit=\"epoch\",\n",
        "    dynamic_ncols=True\n",
        ")\n",
        "\n",
        "# ----------------\n",
        "# Training loop\n",
        "# ----------------\n",
        "for epoch in epoch_bar:\n",
        "    epoch_start = time.time()\n",
        "\n",
        "    # ---- Train ----\n",
        "    train_loss = train_one_epoch(\n",
        "        model=model,\n",
        "        dataloader=train_loader,\n",
        "        optimizer=optimizer,\n",
        "        loss_fn=weighted_loss_fn,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    # ---- Validate ----\n",
        "    val_acc, val_f1 = evaluate(\n",
        "        model=model,\n",
        "        dataloader=val_loader,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    epoch_time = time.time() - epoch_start\n",
        "\n",
        "    # ---- Save metrics ----\n",
        "    history[\"train_loss\"].append(train_loss)\n",
        "    history[\"val_acc\"].append(val_acc)\n",
        "    history[\"val_f1\"].append(val_f1)\n",
        "\n",
        "    # ---- Update progress bar (installer info) ----\n",
        "    epoch_bar.set_postfix({\n",
        "        \"loss\": f\"{train_loss:.4f}\",\n",
        "        \"val_f1\": f\"{val_f1:.4f}\",\n",
        "        \"epoch_time\": format_time(epoch_time)\n",
        "    })\n",
        "\n",
        "# ----------------\n",
        "# Final time log\n",
        "# ----------------\n",
        "total_time = time.time() - start_time\n",
        "\n",
        "print(\"\\nTraining completed successfully.\")\n",
        "print(\"Total training time:\", format_time(total_time))"
      ],
      "metadata": {
        "id": "w6Sl3Gcy-3Mb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving final model and training logs\n",
        "\n",
        "We save:\n",
        "- trained model weights\n",
        "- training history (loss and metrics)\n",
        "\n",
        "This ensures reproducibility and allows later analysis."
      ],
      "metadata": {
        "id": "2Gp-dLjr_JZ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FINAL_MODEL_PATH = \"/content/drive/MyDrive/DNLP/checkpoints/bert_full_training.pt\"\n",
        "HISTORY_PATH = \"/content/drive/MyDrive/DNLP/checkpoints/bert_training_history.pt\"\n",
        "\n",
        "torch.save(model.state_dict(), FINAL_MODEL_PATH)\n",
        "torch.save(history, HISTORY_PATH)\n",
        "\n",
        "print(\"Final model saved to:\", FINAL_MODEL_PATH)\n",
        "print(\"Training history saved to:\", HISTORY_PATH)"
      ],
      "metadata": {
        "id": "FDXkz2Kv-7h8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "assert os.path.exists(FINAL_MODEL_PATH)\n",
        "assert os.path.exists(HISTORY_PATH)\n",
        "print(\"Saved files verified.\")"
      ],
      "metadata": {
        "id": "_X5OG2bU_fdR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Per-variety evaluation\n",
        "\n",
        "We evaluate the trained model separately for each English variety.\n",
        "The model is trained once on all data; only evaluation is grouped by variety."
      ],
      "metadata": {
        "id": "s9l8JDF2JRuh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def evaluate_per_variety(model, dataloader, device):\n",
        "    model.eval()\n",
        "\n",
        "    # Store logits, labels, and varieties\n",
        "    all_logits = []\n",
        "    all_labels = []\n",
        "    all_varieties = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "            varieties = batch[\"variety\"]  # list of strings\n",
        "\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask\n",
        "            )\n",
        "\n",
        "            all_logits.append(outputs.logits.cpu())\n",
        "            all_labels.append(labels.cpu())\n",
        "            all_varieties.extend(varieties)\n",
        "\n",
        "    all_logits = torch.cat(all_logits, dim=0)\n",
        "    all_labels = torch.cat(all_labels, dim=0)\n",
        "\n",
        "    # Group indices by variety\n",
        "    variety_indices = defaultdict(list)\n",
        "    for idx, var in enumerate(all_varieties):\n",
        "        variety_indices[var].append(idx)\n",
        "\n",
        "    # Compute metrics per variety\n",
        "    results = {}\n",
        "    for var, indices in variety_indices.items():\n",
        "        var_logits = all_logits[indices]\n",
        "        var_labels = all_labels[indices]\n",
        "\n",
        "        acc, f1 = compute_metrics(var_logits, var_labels)\n",
        "        results[var] = {\n",
        "            \"accuracy\": acc,\n",
        "            \"macro_f1\": f1,\n",
        "            \"num_samples\": len(indices)\n",
        "        }\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "L0E3E0AvJSHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check that varieties exist and align\n",
        "test_varieties = encoder_data[\"test\"][\"variety\"]\n",
        "\n",
        "print(\"Number of test samples:\", len(test_varieties))\n",
        "print(\"Unique varieties:\", set(test_varieties))"
      ],
      "metadata": {
        "id": "gjU__KYtJVQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running per-variety evaluation on the test set"
      ],
      "metadata": {
        "id": "NHERV5RVJY2X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = BertDataset(encoder_data[\"test\"])\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "per_variety_results = evaluate_per_variety(\n",
        "    model=model,\n",
        "    dataloader=test_loader,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "per_variety_results"
      ],
      "metadata": {
        "id": "mkPDPwvxJZM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Per-variety results summary\n"
      ],
      "metadata": {
        "id": "tNFq1pHUJhP1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for var, metrics in per_variety_results.items():\n",
        "    print(f\"\\nVariety: {var}\")\n",
        "    print(f\"  Samples: {metrics['num_samples']}\")\n",
        "    print(f\"  Accuracy: {metrics['accuracy']:.4f}\")\n",
        "    print(f\"  Macro F1: {metrics['macro_f1']:.4f}\")"
      ],
      "metadata": {
        "id": "s-sYaUZpJeR6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}