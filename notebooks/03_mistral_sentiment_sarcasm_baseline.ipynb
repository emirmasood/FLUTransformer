{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# BESSTIE Benchmark Fine-Tuning with Mistral-2B-Instruct\n",
                "\n",
                "This notebook replicates the BESSTIE benchmark fine-tuning using Mistral-2B-Instruct with QLoRA quantization.\n",
                "\n",
                "## Task Overview:\n",
                "- Fine-tune Mistral-2B-Instruct for classification across three English varieties: en-AU, en-IN, en-UK\n",
                "- Handle three data sections: google-sentiment, reddit-sentiment, reddit-sarcasm\n",
                "- Cross-variety experimental loop with evaluation\n",
                "- Generate 3x3 heatmaps showing F1-scores"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Section 1: Setup and Imports"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install required packages (run once)\n",
                "# !pip install transformers peft bitsandbytes datasets scikit-learn accelerate torch"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import json\n",
                "import os\n",
                "from tqdm.auto import tqdm\n",
                "\n",
                "from transformers import (\n",
                "    AutoTokenizer,\n",
                "    AutoModelForCausalLM,\n",
                "    BitsAndBytesConfig,\n",
                "    TrainingArguments,\n",
                "    Trainer,\n",
                ")\n",
                "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
                "from datasets import load_dataset, Dataset\n",
                "from sklearn.metrics import precision_recall_fscore_support, f1_score\n",
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "# Set device\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "print(f\"Using device: {device}\")\n",
                "\n",
                "# Set random seeds\n",
                "SEED = 50\n",
                "torch.manual_seed(SEED)\n",
                "np.random.seed(SEED)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Section 2: Configuration and Constants"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Model configuration\n",
                "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"  # Or use appropriate Mistral-2B model\n",
                "OUTPUT_DIR = \"./mistral_besstie_outputs\"\n",
                "RESULTS_DIR = \"./mistral_besstie_results\"\n",
                "\n",
                "# Create directories\n",
                "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
                "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
                "\n",
                "# Varieties and tasks\n",
                "VARIETIES = [\"en-AU\", \"en-IN\", \"en-UK\"]\n",
                "TASKS = [\"google-sentiment\", \"reddit-sentiment\", \"reddit-sarcasm\"]\n",
                "\n",
                "# Training hyperparameters\n",
                "MAX_EPOCHS = 30\n",
                "LEARNING_RATE = 2e-4\n",
                "BATCH_SIZE = 16\n",
                "EARLY_STOPPING_PATIENCE = 3\n",
                "EARLY_STOPPING_THRESHOLD = 0.1  # 10% improvement threshold\n",
                "\n",
                "# LoRA hyperparameters\n",
                "LORA_R = 16\n",
                "LORA_ALPHA = 32\n",
                "LORA_DROPOUT = 0.1\n",
                "\n",
                "# Prompts\n",
                "SENTIMENT_PROMPT = \"Generate the sentiment of the given text. 1 for positive sentiment, and 0 for negative sentiment. Do not give an explanation.\"\n",
                "SARCASM_PROMPT = \"Predict if the given text is sarcastic. 1 if the text is sarcastic, and 0 if the text is not sarcastic. Do not give an explanation.\"\n",
                "\n",
                "print(\"Configuration loaded successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Section 3: Data Loading and Preparation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load BESSTIE dataset\n",
                "dataset = load_dataset(\"unswnlporg/BESSTIE\")\n",
                "\n",
                "print(\"Dataset loaded successfully!\")\n",
                "print(f\"Train samples: {len(dataset['train'])}\")\n",
                "print(f\"Test samples: {len(dataset['test'])}\")\n",
                "print(f\"\\nDataset features: {dataset['train'].features}\")\n",
                "print(f\"\\nSample entry: {dataset['train'][0]}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_stratified_splits(dataset, variety, task, val_size=0.1):\n",
                "    \"\"\"\n",
                "    Create stratified train/validation splits for a specific variety and task.\n",
                "    \n",
                "    Args:\n",
                "        dataset: HuggingFace dataset\n",
                "        variety: One of [\"en-AU\", \"en-IN\", \"en-UK\"]\n",
                "        task: One of [\"google-sentiment\", \"reddit-sentiment\", \"reddit-sarcasm\"]\n",
                "        val_size: Validation split size (default: 0.1 for 10%)\n",
                "    \n",
                "    Returns:\n",
                "        train_data, val_data, test_data\n",
                "    \"\"\"\n",
                "    # Filter by variety and task\n",
                "    train_df = dataset['train'].to_pandas()\n",
                "    test_df = dataset['test'].to_pandas()\n",
                "    \n",
                "    # Map task names\n",
                "    task_mapping = {\n",
                "        \"google-sentiment\": \"Sentiment\",\n",
                "        \"reddit-sentiment\": \"Sentiment\",\n",
                "        \"reddit-sarcasm\": \"Sarcasm\"\n",
                "    }\n",
                "    \n",
                "    # Determine source filter\n",
                "    if \"google\" in task:\n",
                "        source_filter = \"Google\"\n",
                "    else:\n",
                "        source_filter = \"Reddit\"\n",
                "    \n",
                "    task_type = task_mapping[task]\n",
                "    \n",
                "    # Filter training data\n",
                "    train_filtered = train_df[\n",
                "        (train_df['variety'] == variety) & \n",
                "        (train_df['task'] == task_type) &\n",
                "        (train_df['source'] == source_filter)\n",
                "    ].copy()\n",
                "    \n",
                "    # Filter test data\n",
                "    test_filtered = test_df[\n",
                "        (test_df['variety'] == variety) & \n",
                "        (test_df['task'] == task_type) &\n",
                "        (test_df['source'] == source_filter)\n",
                "    ].copy()\n",
                "    \n",
                "    # Create stratified train/val split\n",
                "    if len(train_filtered) > 0:\n",
                "        train_data, val_data = train_test_split(\n",
                "            train_filtered,\n",
                "            test_size=val_size,\n",
                "            stratif=train_filtered['label'],\n",
                "            random_state=SEED\n",
                "        )\n",
                "    else:\n",
                "        train_data = train_filtered\n",
                "        val_data = pd.DataFrame()\n",
                "    \n",
                "    return train_data, val_data, test_filtered\n",
                "\n",
                "print(\"Data splitting function defined!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Section 4: Tokenizer and Model Setup with QLoRA"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load tokenizer\n",
                "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
                "tokenizer.pad_token = tokenizer.eos_token\n",
                "tokenizer.padding_side = \"right\"\n",
                "\n",
                "print(\"Tokenizer loaded successfully!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_quantized_model():\n",
                "    \"\"\"\n",
                "    Load model with 4-bit quantization (QLoRA configuration).\n",
                "    \"\"\"\n",
                "    # BitsAndBytes configuration for 4-bit quantization\n",
                "    bnb_config = BitsAndBytesConfig(\n",
                "        load_in_4bit=True,\n",
                "        bnb_4bit_use_double_quant=True,  # Double quantization\n",
                "        bnb_4bit_quant_type=\"nf4\",  # NF4 quantization\n",
                "        bnb_4bit_compute_dtype=torch.bfloat16  # bfloat16 compute\n",
                "    )\n",
                "    \n",
                "    # Load model\n",
                "    model = AutoModelForCausalLM.from_pretrained(\n",
                "        MODEL_NAME,\n",
                "        quantization_config=bnb_config,\n",
                "        device_map=\"auto\",\n",
                "        trust_remote_code=True\n",
                "    )\n",
                "    \n",
                "    # Prepare model for k-bit training\n",
                "    model = prepare_model_for_kbit_training(model)\n",
                "    \n",
                "    return model\n",
                "\n",
                "print(\"Model loading function defined!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def add_lora_adapters(model):\n",
                "    \"\"\"\n",
                "    Add LoRA adapters to all linear layers.\n",
                "    Target layers: q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj\n",
                "    \"\"\"\n",
                "    lora_config = LoraConfig(\n",
                "        r=LORA_R,\n",
                "        lora_alpha=LORA_ALPHA,\n",
                "        target_modules=[\n",
                "            \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
                "            \"gate_proj\", \"up_proj\", \"down_proj\"\n",
                "        ],\n",
                "        lora_dropout=LORA_DROPOUT,\n",
                "        bias=\"none\",\n",
                "        task_type=\"CAUSAL_LM\"\n",
                "    )\n",
                "    \n",
                "    model = get_peft_model(model, lora_config)\n",
                "    model.print_trainable_parameters()\n",
                "    \n",
                "    return model\n",
                "\n",
                "print(\"LoRA adapter function defined!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Section 5: Instruction Formatting and Dataset Preparation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def format_instruction(text, label, task_type):\n",
                "    \"\"\"\n",
                "    Format text into instruction-following format for causal LM.\n",
                "    \n",
                "    Args:\n",
                "        text: Input text\n",
                "        label: Ground truth label (0 or 1)\n",
                "        task_type: \"sentiment\" or \"sarcasm\"\n",
                "    \n",
                "    Returns:\n",
                "        Formatted instruction string\n",
                "    \"\"\"\n",
                "    prompt = SENTIMENT_PROMPT if \"sentiment\" in task_type else SARCASM_PROMPT\n",
                "    \n",
                "    instruction = f\"\"\"<s>[INST] {prompt}\n",
                "\n",
                "Text: {text}\n",
                "[/INST] {label}</s>\"\"\"\n",
                "    \n",
                "    return instruction\n",
                "\n",
                "def tokenize_function(examples, task_type):\n",
                "    \"\"\"\n",
                "    Tokenize examples for training.\n",
                "    \"\"\"\n",
                "    instructions = [\n",
                "        format_instruction(text, label, task_type)\n",
                "        for text, label in zip(examples['text'], examples['label'])\n",
                "    ]\n",
                "    \n",
                "    tokenized = tokenizer(\n",
                "        instructions,\n",
                "        truncation=True,\n",
                "        max_length=512,\n",
                "        padding=\"max_length\",\n",
                "        return_tensors=\"pt\"\n",
                "    )\n",
                "    \n",
                "    # For causal LM, labels are the same as input_ids\n",
                "    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n",
                "    \n",
                "    return tokenized\n",
                "\n",
                "print(\"Instruction formatting functions defined!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Section 6: Training Loop with Early Stopping"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class EarlyStoppingCallback:\n",
                "    \"\"\"\n",
                "    Custom early stopping callback.\n",
                "    Stops training if validation loss doesn't improve by threshold for patience epochs.\n",
                "    \"\"\"\n",
                "    def __init__(self, patience=3, threshold=0.1):\n",
                "        self.patience = patience\n",
                "        self.threshold = threshold\n",
                "        self.best_loss = float('inf')\n",
                "        self.counter = 0\n",
                "        self.should_stop = False\n",
                "    \n",
                "    def __call__(self, val_loss):\n",
                "        improvement = (self.best_loss - val_loss) / self.best_loss if self.best_loss != float('inf') else 0\n",
                "        \n",
                "        if improvement >= self.threshold:\n",
                "            self.best_loss = val_loss\n",
                "            self.counter = 0\n",
                "        else:\n",
                "            self.counter += 1\n",
                "        \n",
                "        if self.counter >= self.patience:\n",
                "            self.should_stop = True\n",
                "        \n",
                "        return self.should_stop\n",
                "\n",
                "print(\"Early stopping callback defined!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Section 7: Evaluation Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def extract_prediction(output_text):\n",
                "    \"\"\"\n",
                "    Extract prediction (0 or 1) from model output.\n",
                "    \"\"\"\n",
                "    # Look for the last occurrence of 0 or 1\n",
                "    text = output_text.strip()\n",
                "    if '1' in text:\n",
                "        return 1\n",
                "    elif '0' in text:\n",
                "        return 0\n",
                "    else:\n",
                "        # Default to 0 if unclear\n",
                "        return 0\n",
                "\n",
                "def evaluate_model(model, test_data, task_type):\n",
                "    \"\"\"\n",
                "    Evaluate model on test data.\n",
                "    \n",
                "    Returns:\n",
                "        precision, recall, f1_score (macro-averaged)\n",
                "    \"\"\"\n",
                "    model.eval()\n",
                "    predictions = []\n",
                "    true_labels = []\n",
                "    \n",
                "    prompt = SENTIMENT_PROMPT if \"sentiment\" in task_type else SARCASM_PROMPT\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        for _, row in tqdm(test_data.iterrows(), total=len(test_data), desc=\"Evaluating\"):\n",
                "            instruction = f\"\"\"<s>[INST] {prompt}\n",
                "\n",
                "Text: {row['text']}\n",
                "[/INST]\"\"\"\n",
                "            \n",
                "            inputs = tokenizer(instruction, return_tensors=\"pt\").to(device)\n",
                "            outputs = model.generate(\n",
                "                **inputs,\n",
                "                max_new_tokens=5,\n",
                "                temperature=0.1,\n",
                "                do_sample=False\n",
                "            )\n",
                "            \n",
                "            output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
                "            pred = extract_prediction(output_text)\n",
                "            \n",
                "            predictions.append(pred)\n",
                "            true_labels.append(row['label'])\n",
                "    \n",
                "    # Calculate metrics\n",
                "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
                "        true_labels, predictions, average='macro', zero_division=0\n",
                "    )\n",
                "    \n",
                "    return precision, recall, f1\n",
                "\n",
                "print(\"Evaluation functions defined!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Section 8: Main Training Loop (Cross-Variety Experiment)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Storage for all results\n",
                "all_results = []\n",
                "\n",
                "# Iterate through all tasks\n",
                "for task in TASKS:\n",
                "    print(f\"\\n{'='*80}\")\n",
                "    print(f\"Processing Task: {task}\")\n",
                "    print(f\"{'='*80}\\n\")\n",
                "    \n",
                "    task_results = {}\n",
                "    \n",
                "    # Train on each variety\n",
                "    for train_variety in VARIETIES:\n",
                "        print(f\"\\n--- Training on {train_variety} ---\")\n",
                "        \n",
                "        # Prepare data\n",
                "        train_data, val_data, _ = create_stratified_splits(dataset, train_variety, task)\n",
                "        \n",
                "        if len(train_data) == 0:\n",
                "            print(f\"No training data for {train_variety} - {task}. Skipping...\")\n",
                "            continue\n",
                "        \n",
                "        # Convert to HF Dataset\n",
                "        train_dataset = Dataset.from_pandas(train_data)\n",
                "        val_dataset = Dataset.from_pandas(val_data) if len(val_data) > 0 else None\n",
                "        \n",
                "        # Tokenize\n",
                "        train_dataset = train_dataset.map(\n",
                "            lambda x: tokenize_function(x, task),\n",
                "            batched=True,\n",
                "            remove_columns=train_dataset.column_names\n",
                "        )\n",
                "        \n",
                "        if val_dataset:\n",
                "            val_dataset = val_dataset.map(\n",
                "                lambda x: tokenize_function(x, task),\n",
                "                batched=True,\n",
                "                remove_columns=val_dataset.column_names\n",
                "            )\n",
                "        \n",
                "        # Load model\n",
                "        model = get_quantized_model()\n",
                "        model = add_lora_adapters(model)\n",
                "        \n",
                "        # Training arguments\n",
                "        training_args = TrainingArguments(\n",
                "            output_dir=f\"{OUTPUT_DIR}/{task}_{train_variety}\",\n",
                "            num_train_epochs=MAX_EPOCHS,\n",
                "            per_device_train_batch_size=BATCH_SIZE,\n",
                "            per_device_eval_batch_size=BATCH_SIZE,\n",
                "            learning_rate=LEARNING_RATE,\n",
                "            warmup_steps=100,\n",
                "            logging_steps=10,\n",
                "            evaluation_strategy=\"epoch\",\n",
                "            save_strategy=\"epoch\",\n",
                "            load_best_model_at_end=True,\n",
                "            optim=\"paged_adamw_8bit\",\n",
                "            fp16=True,\n",
                "            gradient_accumulation_steps=4,\n",
                "            report_to=\"none\"\n",
                "        )\n",
                "        \n",
                "        # Trainer\n",
                "        trainer = Trainer(\n",
                "            model=model,\n",
                "            args=training_args,\n",
                "            train_dataset=train_dataset,\n",
                "            eval_dataset=val_dataset\n",
                "        )\n",
                "        \n",
                "        # Train\n",
                "        print(\"Starting training...\")\n",
                "        trainer.train()\n",
                "        \n",
                "        # Save best model adapters\n",
                "        model.save_pretrained(f\"{OUTPUT_DIR}/{task}_{train_variety}_best\")\n",
                "        \n",
                "        # Evaluate on all varieties\n",
                "        print(f\"\\nEvaluating {train_variety} model on all varieties...\")\n",
                "        \n",
                "        for test_variety in VARIETIES:\n",
                "            _, _, test_data = create_stratified_splits(dataset, test_variety, task)\n",
                "            \n",
                "            if len(test_data) == 0:\n",
                "                continue\n",
                "            \n",
                "            precision, recall, f1 = evaluate_model(model, test_data, task)\n",
                "            \n",
                "            result = {\n",
                "                \"task\": task,\n",
                "                \"trained_on\": train_variety,\n",
                "                \"tested_on\": test_variety,\n",
                "                \"precision\": precision,\n",
                "                \"recall\": recall,\n",
                "                \"f1_score\": f1\n",
                "            }\n",
                "            \n",
                "            all_results.append(result)\n",
                "            \n",
                "            print(f\"{train_variety} → {test_variety}: Precision={precision:.4f}, Recall={recall:.4f}, F1={f1:.4f}\")\n",
                "        \n",
                "        # Clean up\n",
                "        del model\n",
                "        del trainer\n",
                "        torch.cuda.empty_cache()\n",
                "\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"All training and evaluation completed!\")\n",
                "print(\"=\"*80)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Section 9: Save Results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Convert results to DataFrame\n",
                "results_df = pd.DataFrame(all_results)\n",
                "\n",
                "# Save to CSV\n",
                "results_df.to_csv(f\"{RESULTS_DIR}/mistral_besstie_results.csv\", index=False)\n",
                "\n",
                "# Save to JSON\n",
                "with open(f\"{RESULTS_DIR}/mistral_besstie_results.json\", 'w') as f:\n",
                "    json.dump(all_results, f, indent=2)\n",
                "\n",
                "print(\"Results saved successfully!\")\n",
                "print(f\"\\nResults DataFrame:\")\n",
                "print(results_df)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Section 10: Generate Heatmaps"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate heatmaps for each task\n",
                "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
                "\n",
                "for idx, task in enumerate(TASKS):\n",
                "    # Filter results for this task\n",
                "    task_df = results_df[results_df['task'] == task]\n",
                "    \n",
                "    # Create pivot table for heatmap\n",
                "    heatmap_data = task_df.pivot(\n",
                "        index='trained_on',\n",
                "        columns='tested_on',\n",
                "        values='f1_score'\n",
                "    )\n",
                "    \n",
                "    # Ensure correct order\n",
                "    heatmap_data = heatmap_data.reindex(index=VARIETIES, columns=VARIETIES)\n",
                "    \n",
                "    # Plot heatmap\n",
                "    sns.heatmap(\n",
                "        heatmap_data,\n",
                "        annot=True,\n",
                "        fmt='.3f',\n",
                "        cmap='RdYlGn',\n",
                "        vmin=0,\n",
                "        vmax=1,\n",
                "        ax=axes[idx],\n",
                "        cbar_kws={'label': 'F1 Score'},\n",
                "        square=True\n",
                "    )\n",
                "    \n",
                "    axes[idx].set_title(f\"{task.replace('-', ' ').title()}\", fontsize=14, fontweight='bold')\n",
                "    axes[idx].set_xlabel('Tested On', fontsize=12)\n",
                "    axes[idx].set_ylabel('Trained On', fontsize=12)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig(f\"{RESULTS_DIR}/mistral_besstie_heatmaps.png\", dpi=300, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "print(\"Heatmaps generated and saved!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Section 11: Summary Statistics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate summary statistics\n",
                "print(\"\\nSummary Statistics:\\n\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "for task in TASKS:\n",
                "    task_df = results_df[results_df['task'] == task]\n",
                "    \n",
                "    print(f\"\\n{task.upper()}:\")\n",
                "    print(\"-\" * 40)\n",
                "    \n",
                "    # Same-variety performance\n",
                "    same_variety = task_df[task_df['trained_on'] == task_df['tested_on']]\n",
                "    print(f\"Same-variety F1 (mean): {same_variety['f1_score'].mean():.4f}\")\n",
                "    \n",
                "    # Cross-variety performance\n",
                "    cross_variety = task_df[task_df['trained_on'] != task_df['tested_on']]\n",
                "    print(f\"Cross-variety F1 (mean): {cross_variety['f1_score'].mean():.4f}\")\n",
                "    \n",
                "    # Best and worst\n",
                "    best_idx = task_df['f1_score'].idxmax()\n",
                "    worst_idx = task_df['f1_score'].idxmin()\n",
                "    \n",
                "    best = task_df.loc[best_idx]\n",
                "    worst = task_df.loc[worst_idx]\n",
                "    \n",
                "    print(f\"Best: {best['trained_on']} → {best['tested_on']}: {best['f1_score']:.4f}\")\n",
                "    print(f\"Worst: {worst['trained_on']} → {worst['tested_on']}: {worst['f1_score']:.4f}\")\n",
                "\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"Analysis Complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Conclusion\n",
                "\n",
                "This notebook has successfully replicated the BESSTIE benchmark fine-tuning process using Mistral-2B-Instruct with QLoRA quantization. The results show cross-variety performance across three English varieties and three different tasks.\n",
                "\n",
                "### Key Outputs:\n",
                "1. Trained LoRA adapters saved in `./mistral_besstie_outputs/`\n",
                "2. Results CSV and JSON saved in `./mistral_besstie_results/`\n",
                "3. Heatmap visualizations showing F1-scores for all variety combinations\n",
                "4. Summary statistics comparing same-variety vs cross-variety performance"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
